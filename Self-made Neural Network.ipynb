{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b413608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43ac5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer, n_classes, lr=0.005, alpha=0.7):\n",
    "        self.w1 = np.random.normal(0, 1, size=(input_layer, hidden_layer))\n",
    "        self.w2 = np.random.normal(0, 1, size=(hidden_layer, output_layer))\n",
    "        self.w3 = np.random.normal(0, 1, size=(output_layer, n_classes))\n",
    "        self.b1 = np.random.normal(0, 1, size=(1, hidden_layer))\n",
    "        self.b2 = np.random.normal(0, 1, size=(1, output_layer))\n",
    "        self.b3 = np.random.normal(0, 1, size=(1, n_classes))\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def activation_function(self, vec):\n",
    "        relu = np.vectorize(lambda x: max(0, x))\n",
    "        return relu(vec)\n",
    "    \n",
    "    def derivative_function(self, vec):\n",
    "        relu_df = np.vectorize(lambda x: 1 if x >= 0 else 0)\n",
    "        return relu_df(vec)\n",
    "    \n",
    "    def softmax(self, vec):\n",
    "        vec = np.exp(vec)\n",
    "        sum_exp = np.sum(vec)\n",
    "        return vec / sum_exp\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = [self.w1, self.w2]\n",
    "        bias = [self.b1, self.b2]\n",
    "        inputs, outputs = [], [x.reshape((1, -1))] \n",
    "        for w, b in zip(weights, bias):\n",
    "            x = x @ w + b\n",
    "            inputs.append(x)\n",
    "            x = self.activation_function(x)\n",
    "            outputs.append(x)\n",
    "        x = x @ self.w3 + self.b3\n",
    "        proba = self.softmax(x)\n",
    "        return inputs, outputs, proba\n",
    "    \n",
    "    def train(self, X, y, epochs=20, batch=64):\n",
    "        momentum_w = [np.zeros(w.shape) for w in (self.w1, self.w2, self.w3)]\n",
    "        momentum_b = [np.zeros(b.shape) for b in (self.b1, self.b2, self.b3)]\n",
    "        for k in tqdm(range(epochs)):\n",
    "            indx = np.random.permutation(len(y))\n",
    "            X, y = X[indx], y[indx]\n",
    "            for i in range(len(y) // batch + bool(len(y) % batch)):\n",
    "                X_batch, y_batch = X[i*batch:(i+1)*batch], y[i*batch:(i+1)*batch]\n",
    "                dw = [np.zeros(w.shape) for w in (self.w1, self.w2, self.w3)]\n",
    "                db = [np.zeros(b.shape) for b in (self.b1, self.b2, self.b3)]\n",
    "                for x_i, y_i in zip(X_batch, y_batch):\n",
    "                    inputs, outputs, df = self.forward(x_i)\n",
    "\n",
    "                    df[0, y_i] -= 1\n",
    "                    dw[-1] += outputs[-1].T @ df\n",
    "                    db[-1] += df\n",
    "\n",
    "                    df = (df @ self.w3.T) * self.derivative_function(inputs[-1])\n",
    "                    dw[-2] += outputs[-2].T @ df\n",
    "                    db[-2] += df\n",
    "                    \n",
    "                    df = (df @ self.w2.T) * self.derivative_function(inputs[-2])\n",
    "                    dw[-3] += outputs[-3].T @ df\n",
    "                    db[-3] += df\n",
    "                \n",
    "                for j in range(len(momentum_w)):\n",
    "                    momentum_w[j] = self.alpha * momentum_w[j] + (1 - self.alpha) * dw[j]\n",
    "                    momentum_b[j] = self.alpha * momentum_b[j] + (1 - self.alpha) * db[j]\n",
    "                \n",
    "                self.w1 -= self.lr * momentum_w[0]\n",
    "                self.w2 -= self.lr * momentum_w[1]\n",
    "                self.w3 -= self.lr * momentum_w[2] \n",
    "                self.b1 -= self.lr * momentum_b[0]\n",
    "                self.b2 -= self.lr * momentum_b[1]\n",
    "                self.b3 -= self.lr * momentum_b[2]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = np.array([np.argmax(self.forward(x)[2]) for x in X])\n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1d3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    _type = \"activation_function\"\n",
    "    def activation(self, vec):\n",
    "        relu = np.vectorize(lambda x: max(0, x))\n",
    "        return relu(vec)\n",
    "    \n",
    "    def derivative(self, vec):\n",
    "        relu_df = np.vectorize(lambda x: 1 if x >= 0 else 0)\n",
    "        return relu_df(vec) \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    _type = \"loss\"\n",
    "    def loss(self, vec):\n",
    "        vec = np.exp(vec)\n",
    "        sum_exp = np.sum(vec)\n",
    "        return vec / sum_exp\n",
    "    \n",
    "    def derivative(self, p, y):\n",
    "        p[0, y] -= 1\n",
    "        return p\n",
    "\n",
    "class Linear:\n",
    "    _type = \"linear\"\n",
    "    def __init__(self, inp, outp):\n",
    "        self.weight = np.random.normal(0, 1, size=(inp, outp))\n",
    "        self.bias = np.random.normal(0, 1, size=(1, outp))\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, *args):\n",
    "        self.layers = args\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((1, -1))\n",
    "        inputs, outputs = [], [x]\n",
    "        for layer in self.layers:\n",
    "            if layer._type == \"loss\":\n",
    "                proba = layer.loss(x)\n",
    "            elif layer._type == \"linear\":\n",
    "                x = x @ layer.weight + layer.bias\n",
    "                inputs.append(x)\n",
    "            elif layer._type == \"activation_function\":\n",
    "                x = layer.activation(x)\n",
    "                outputs.append(x)\n",
    "        return inputs, outputs, proba\n",
    "    \n",
    "    def train(self, X, y, lr=0.003, epochs=40, batch=64):\n",
    "        linear = [l for l in self.layers if l._type == \"linear\"]\n",
    "        for _ in tqdm(range(epochs)):\n",
    "            indx = np.random.permutation(len(y))\n",
    "            X, y = X[indx], y[indx]\n",
    "            for i in range(len(y) // batch + bool(len(y) % batch)):\n",
    "                X_batch, y_batch = X[i*batch:(i+1)*batch], y[i*batch:(i+1)*batch]\n",
    "                dw = [np.zeros(l.weight.shape) for l in linear]\n",
    "                db = [np.zeros(l.bias.shape) for l in linear]\n",
    "                for x_i, y_i in zip(X_batch, y_batch):\n",
    "                    inputs, outputs, df = self.forward(x_i)\n",
    "                    output_ind, input_ind = -2, -2\n",
    "                    for layer in self.layers[::-1]:\n",
    "                        if layer._type == \"loss\":\n",
    "                            df = layer.derivative(df, y_i)\n",
    "                            dw[-1] += outputs[-1].T @ df\n",
    "                            db[-1] += df\n",
    "                        elif layer._type == \"linear\":\n",
    "                            df = (df @ layer.weight.T) \n",
    "                        elif layer._type == \"activation_function\":\n",
    "                            df = df * layer.derivative(inputs[input_ind])\n",
    "                            dw[output_ind] += outputs[output_ind].T @ df\n",
    "                            db[output_ind] += df\n",
    "                            output_ind -= 1\n",
    "                            input_ind -= 1\n",
    "                \n",
    "                for i in range(len(linear)):\n",
    "                    linear[i].weight -= lr * dw[i]\n",
    "                    linear[i].bias -= lr * db[i]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = np.array([np.argmax(self.forward(x)[2]) for x in X])\n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eae75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(10000, 5, n_classes=3, n_informative=5, n_redundant=0, random_state=17)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=17, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32add39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(5, 6, 8, 10)\n",
    "nn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eea8b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8546666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f69b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(5, 20),\n",
    "          ReLU(),\n",
    "          Linear(20, 20),\n",
    "          ReLU(),\n",
    "          Linear(20, 10),\n",
    "          CrossEntropyLoss()]\n",
    "\n",
    "model = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab9d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:48<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e76e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afb663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
